# SPL225 - Assignment 2: GurionRock Pro Max Ultra Over 9000

> **Concurrent Perception & Mapping Simulation Framework in Java**

## ğŸš€ Overview

This project simulates the perception and mapping system of a smart robotic vacuum cleaner, modeled after real-world SLAM (Simultaneous Localization and Mapping) systems. The system is built in Java using **concurrent microservices** communicating via an event-driven **MessageBus** framework. Sensors like **Cameras**, **LiDAR**, **GPS**, and **IMU** operate in parallel and contribute data for environment mapping.

This project was implemented as part of the SPL225 course at Ben-Gurion University.

## ğŸ§  Core Concepts

- **Java Concurrency**: Threads, synchronization, lambdas, callbacks.
- **MicroServices Architecture**: Event-based message passing with support for broadcast and future resolution.
- **Sensor Fusion & SLAM**: Combines data from multiple sensors to build a world map.
- **Event-Driven Simulation**: Time-driven system using broadcasts, event queues, and worker threads.

## ğŸ› ï¸ Features

- âœ… Custom `Future<T>` implementation.
- âœ… Thread-safe `MessageBus` with round-robin event distribution.
- âœ… Camera and LiDAR workers simulate real-time detection and tracking.
- âœ… `FusionSLAM` component integrates sensor data into a global map.
- âœ… Error handling for faulty sensors and crash propagation.
- âœ… JSON-based configuration and input/output.
- âœ… Maven project structure with JUnit test coverage.

## ğŸ“ Project Structure

```text
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â””â”€â”€ java/
â”‚   â”‚       â””â”€â”€ bgu/spl/mics/         # MicroServices framework
â”‚   â”‚       â””â”€â”€ bgu/spl/components/   # Camera, LiDAR, FusionSLAM, etc.
â”‚   â”‚       â””â”€â”€ bgu/spl/messages/     # Events and Broadcasts
â”‚   â”‚       â””â”€â”€ bgu/spl/util/         # Utility classes (Pose, Landmark, etc.)
â”‚
â”‚   â””â”€â”€ test/
â”‚       â””â”€â”€ java/
â”‚           â””â”€â”€ bgu/spl/tests/        # JUnit test classes
â”‚
â”œâ”€â”€ pom.xml                           # Maven configuration
```

 

## ğŸ“¦ Input Files

The program accepts a **configuration JSON file** as its main input, which includes:

- âœ… Camera configuration and detection data path
- âœ… LiDAR configuration and cloud point path
- âœ… Pose data path
- âœ… Tick time and simulation duration

Additional JSON files describe:

- ğŸ–¼ï¸ Camera-detected objects
- ğŸ“¡ LiDAR-detected cloud points
- ğŸ“ Robot poses at every tick

> See the `input/` folder for example files (not included in the repo due to assignment policies).

## ğŸ“¤ Output

A single `output_file.json` is generated, including:

- ğŸ“Š Runtime statistics (e.g., # of objects detected/tracked)
- ğŸ—ºï¸ Final world map with global coordinates
- âš ï¸ Error report (if a sensor fails)

## ğŸ§ª Testing

Unit tests are implemented using **JUnit 5** for:

- `MessageBusImpl` (thread-safe event handling)
- Camera/LiDAR services (data prep)
- FusionSLAM (coordinate transformation logic)

To run tests:

```bash
mvn test
```

## ğŸ§± Build and Run Instructions

âœ… Prerequisites
Java 8+
Maven

## ğŸ“¦ Build
mvn clean install

## â–¶ï¸ Run
java -jar target/assignment2.jar path/to/config.json
Make sure to run on a UNIX machine.

ğŸ” Coordinate Transformation

## Sensor data from LiDAR is in the robotâ€™s local frame and must be converted to the global frame: ğŸ§® Formula
```text
double yawRad = Math.toRadians(yaw);
double x_global = Math.cos(yawRad) * x_local - Math.sin(yawRad) * y_local + robot_x;
double y_global = Math.sin(yawRad) * x_local + Math.cos(yawRad) * y_local + robot_y;
Used to rotate and translate cloud points based on the robot's pose at detection time.
```
## ğŸ“š Libraries Used
GSON â€“ for parsing JSON

JUnit 5 â€“ for testing

## ğŸ‘¨â€ğŸ’» Authors
[Roei Yosef]

## 
Course: SPL225 â€“ Systems Programming Lab
Institution: Ben-Gurion University of the Negev
Year: 2025
