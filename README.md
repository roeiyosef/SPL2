# SPL225 - Assignment 2: GurionRock Pro Max Ultra Over 9000

> **Concurrent Perception & Mapping Simulation Framework in Java**

## ğŸš€ Overview

This project simulates the perception and mapping system of a smart robotic vacuum cleaner, modeled after real-world SLAM (Simultaneous Localization and Mapping) systems. The system is built in Java using **concurrent microservices** communicating via an event-driven **MessageBus** framework. Sensors like **Cameras**, **LiDAR**, **GPS**, and **IMU** operate in parallel and contribute data for environment mapping.

This project was implemented as part of the SPL225 course at Ben-Gurion University.

## ğŸ§  Core Concepts

- **Java Concurrency**: Threads, synchronization, lambdas, callbacks.
- **MicroServices Architecture**: Event-based message passing with support for broadcast and future resolution.
- **Sensor Fusion & SLAM**: Combines data from multiple sensors to build a world map.
- **Event-Driven Simulation**: Time-driven system using broadcasts, event queues, and worker threads.

## ğŸ› ï¸ Features

- âœ… Custom `Future<T>` implementation.
- âœ… Thread-safe `MessageBus` with round-robin event distribution.
- âœ… Camera and LiDAR workers simulate real-time detection and tracking.
- âœ… `FusionSLAM` component integrates sensor data into a global map.
- âœ… Error handling for faulty sensors and crash propagation.
- âœ… JSON-based configuration and input/output.
- âœ… Maven project structure with JUnit test coverage.

## ğŸ“ Project Structure

.
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ main/
â”‚ â”‚ â”œâ”€â”€ java/
â”‚ â”‚ â”‚ â””â”€â”€ bgu/spl/mics/ # MicroServices framework
â”‚ â”‚ â”‚ â””â”€â”€ bgu/spl/components/ # Camera, LiDAR, FusionSLAM, etc.
â”‚ â”‚ â”‚ â””â”€â”€ bgu/spl/messages/ # Events and Broadcasts
â”‚ â”‚ â”‚ â””â”€â”€ bgu/spl/util/ # Utility classes (Pose, Landmark, etc.)
â”‚ â””â”€â”€ test/
â”‚ â””â”€â”€ java/
â”‚ â””â”€â”€ bgu/spl/tests/ # JUnit test classes
â”œâ”€â”€ pom.xml # Maven configuration

markdown
Copy
Edit

## ğŸ“¦ Input Files

The program accepts a **configuration JSON file** as its main input, which includes:

- âœ… Camera configuration and detection data path
- âœ… LiDAR configuration and cloud point path
- âœ… Pose data path
- âœ… Tick time and simulation duration

Additional JSON files describe:

- ğŸ–¼ï¸ Camera-detected objects
- ğŸ“¡ LiDAR-detected cloud points
- ğŸ“ Robot poses at every tick

> See the `input/` folder for example files (not included in the repo due to assignment policies).

## ğŸ“¤ Output

A single `output_file.json` is generated, including:

- ğŸ“Š Runtime statistics (e.g., # of objects detected/tracked)
- ğŸ—ºï¸ Final world map with global coordinates
- âš ï¸ Error report (if a sensor fails)

## ğŸ§ª Testing

Unit tests were written using **JUnit** for:

- `MessageBusImpl`
- Camera and LiDAR services
- SLAM fusion calculations

To run tests:

```bash
mvn test
ğŸ§± Build and Run Instructions
âœ… Prerequisites
Java 8+

Maven

ğŸ“¦ Build
bash
Copy
Edit
mvn clean install
â–¶ï¸ Run
bash
Copy
Edit
java -jar target/assignment2.jar path/to/config.json
ğŸ” Coordinate Transformation
LiDAR data is initially in the robotâ€™s local frame and must be converted to global coordinates using the robot's pose (x, y, yaw). The transformation includes:

Rotation by yaw

Translation by the robotâ€™s position

java
Copy
Edit
double x_global = cos(yaw) * x_local - sin(yaw) * y_local + robot_x;
double y_global = sin(yaw) * x_local + cos(yaw) * y_local + robot_y;
For more details, see Appendix B in the provided documents.

ğŸ“š Libraries Used
GSON - JSON serialization/deserialization

JUnit 5 - Testing framework

ğŸ§‘â€ğŸ’» Authors
[Your Name]

[Partner's Name (if any)]

Course: SPL225 - Systems Programming Lab
Institution: Ben-Gurion University
Year: 2025
